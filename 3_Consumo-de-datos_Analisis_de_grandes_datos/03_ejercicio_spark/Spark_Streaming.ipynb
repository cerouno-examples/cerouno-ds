{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "\n",
    "\n",
    "\n",
    "Spark Streaming es una extensi√≥n de la API core de Spark que habilita un procesamiento de flujos para canales de datos en vivo, con un soporte escalable, de alto rendimiento y tolerante a fallas. La ingesta de datos puede provenir de distintas fuentes como Kafka, Flume, Kinesis o sockets TCP. Y pueden ser procesados con algoritmos expresados mediante funciones de alto nivel como *map*, *reduce*, *join* y *window*. \n",
    "\n",
    "Finalmente, los datos procesados se pueden guardar en un sistema de archivos, bases de datos y dashboards en vivo. Tambi√©n se dispone la capacidad de usar los algoritmos de Spark para Machine Learning y procesamiento de Grafos sobre los flujos de datos.\n",
    "\n",
    "\n",
    "![Spark Streaming](https://spark.apache.org/docs/latest/img/streaming-arch.png)\n",
    "\n",
    "Internamente, Spark Streaming recibe datos de canales en vivo y los divide en series (batches), que son procesados por el motor de Spark para generar el flujo final en forma de series finales.\n",
    "\n",
    "![Spark Streaming internals](https://spark.apache.org/docs/latest/img/streaming-flow.png)\n",
    "\n",
    "\n",
    "Spark Streaming provee una abstracci√≥n de alto nivel llamada *discretized stream* o *DStream*, que representa una corritente continua de datos. Los DStream, se pueden crear a partir de flujos de datos como Kafka, Flume y Kinesis, o despu√©s de aplicarse operaciones de alto nivel sobre otros DStream. De forma interna, un DStream se representa como una secuencia de RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos StreamingContext, que es el acceso para todas las funcionalidades de Spark Streaming. Creamos una instancia del objeto con dos hilos de ejecuci√≥n, y a 10 segundos como intervalo para la creaci√≥n del batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\", \"NetworkWordCountMarianna\")\n",
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el contecto anterior, creamos un DStream que representa el flujo de datos de una fuente TCP (socket), se especifica un hostname y un puerto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El DStream *lines* representa un flujo de datos que ser√°n recibidos desde otro servidor. Cada registro en este DStream es una l√≠nea de texto. Despu√©s de recibir el registro, separaremos las palabras usando los espacios entre ellas.\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/streaming-dstream.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*flatMap* es una operaci√≥n (transformaci√≥n one-to-many) para DStream que crea un nuevo objeto al generar multiples registros por cada registro en el DStream de la fuente. En este caso, cada l√≠nea ser√° cortada para obtener multiples palabras y representarse en el DStream *words*. Despu√©s, vamos a imprimir esas palabras.\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "El DStream  *words* es transformado (*map*, one-to-one) al siguiente DStream de pares llave-valor con el siguiente formato (palabra, 1), donde se reduce para obtener la frecuencia de palabras en cada batch de datos. Finalmente, *wordCounts.pprint()* imprime el conteo generado en ese intervalo.\n",
    "\n",
    "\n",
    "Debemos recordar, que a√∫n cuando las l√≠neas de c√≥digo anteriores ya fueron ejecutadas, Spark Streaming ejecutar√° el c√≥mputo hasta que el proceso sea requerido. Mientras tanto, no ha habido un procesamiento real de datos.\n",
    "\n",
    "Para iniciar el procesamiento de datos, despu√©s de que las transformaciones fueron establecidas, llamamos a las funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()   # üòâ\n",
    "\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Despliegue del programa de Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ /usr/local/spark/bin/spark-submit network_wordcount.py localhost 9999\n",
    "```\n",
    "\n",
    "Entrar a http://localhost:4040 para ver la interfaz de monitoreo de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Lista completa de las transformaciones para DStreams: https://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams\n",
    "\n",
    "\n",
    "Puntos a  recordar:\n",
    "- Una vez que el contexto ha sido creado, no se pueden agregar nuevas computaciones  o modificar las existentes.\n",
    "- Una vez que el contexto ha sido detenido, no puede reiniciarse.\n",
    "- Solo un contexto de StreamingContext puede estar activo por cada JVM.\n",
    "- stop() en StreamingContext tambi√©n detiene el contexto SparkContext. Para detener solo el contexto StreamingContext,  hay que establecer el par√°metro *stopSparkContext a Falso en stop().\n",
    "- Una instancia de SparkContext puede ser reusada para crear multiples objetos StreamingContext, en el caso de que el contexto de streaming haya sido detenido con anterioridad y sin detener SparkContext"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
